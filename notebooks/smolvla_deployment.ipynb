{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b419b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
    "# from huggingface_hub import snapshot_download\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "\n",
    "TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45f6f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No accelerated backend detected. Using default cpu, this will be slow.\n",
      "WARNING:root:Device 'cuda' is not available. Switching to 'cpu'.\n",
      "WARNING:root:No accelerated backend detected. Using default cpu, this will be slow.\n",
      "WARNING:root:Device 'cuda' is not available. Switching to 'cpu'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
      "Reducing the number of VLM layers to 16 ...\n",
      "Loading weights from local directory\n"
     ]
    }
   ],
   "source": [
    "policy = SmolVLAPolicy.from_pretrained('franka_smolvla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfead36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# from gymnasium import envs\n",
    "# pprint(envs.registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac1c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n",
      "Observation space: Dict('achieved_goal': Box(-10.0, 10.0, (3,), float32), 'desired_goal': Box(-10.0, 10.0, (3,), float32), 'observation': Box(-10.0, 10.0, (19,), float32))\n",
      "Action space: Box(-1.0, 1.0, (4,), float32)\n",
      "Policy input features: {'observation.images.image_additional_view': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 128, 128)), 'observation.images.image': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 128, 128)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(13,))}\n",
      "Policy output features: {'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(15,))}\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.01062162,  0.00829344,  0.00525099,  0.02169818], dtype=float32), array([ 0.01869066,  0.05501999, -0.03474769,  0.01036961], dtype=float32), array([ 0.00286634, -0.00049341, -0.00779088, -0.00519954], dtype=float32), array([ 0.02697577, -0.54684705,  0.00763237], dtype=float32)]\n",
      "Step 0 | Reward: -1.000 | Terminated: False\n",
      "Step 1 | Reward: -1.000 | Terminated: False\n",
      "Step 2 | Reward: -1.000 | Terminated: False\n",
      "Step 3 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.00932299,  0.03894178, -0.02544932,  0.03528485], dtype=float32), array([ 0.00933426,  0.0338446 , -0.02067972,  0.01322222], dtype=float32), array([-0.00330908, -0.00303507,  0.01511847, -0.01187509], dtype=float32), array([ 0.02977839, -0.41134405,  0.00599606], dtype=float32)]\n",
      "Step 4 | Reward: -1.000 | Terminated: False\n",
      "Step 5 | Reward: -1.000 | Terminated: False\n",
      "Step 6 | Reward: -1.000 | Terminated: False\n",
      "Step 7 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.00808222,  0.00400095, -0.01550351,  0.01766371], dtype=float32), array([ 0.02853144,  0.01165091, -0.010216  ,  0.00289803], dtype=float32), array([ 0.0017233 , -0.00514324,  0.01004776, -0.00587818], dtype=float32), array([ 0.01194673, -0.6970462 ,  0.00607144], dtype=float32)]\n",
      "Step 8 | Reward: -1.000 | Terminated: False\n",
      "Step 9 | Reward: -1.000 | Terminated: False\n",
      "Step 10 | Reward: -1.000 | Terminated: False\n",
      "Step 11 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.02779109, -0.02040088,  0.01555131, -0.00157136], dtype=float32), array([ 0.03658186, -0.01994947, -0.03074584,  0.00248003], dtype=float32), array([-0.00493887, -0.00466569, -0.00403443,  0.03787777], dtype=float32), array([ 0.00567198, -0.32611567,  0.00711168], dtype=float32)]\n",
      "Step 12 | Reward: -1.000 | Terminated: False\n",
      "Step 13 | Reward: -1.000 | Terminated: False\n",
      "Step 14 | Reward: -1.000 | Terminated: False\n",
      "Step 15 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.01119039, -0.0054633 , -0.02593908, -0.0137156 ], dtype=float32), array([ 0.02145295, -0.00721095,  0.01917805,  0.00568773], dtype=float32), array([-0.01344722,  0.0010114 , -0.03352188, -0.02022239], dtype=float32), array([-2.9121190e-02, -9.6513623e-01,  1.3887323e-04], dtype=float32)]\n",
      "Step 16 | Reward: -1.000 | Terminated: False\n",
      "Step 17 | Reward: -1.000 | Terminated: False\n",
      "Step 18 | Reward: -1.000 | Terminated: False\n",
      "Step 19 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.00131057,  0.00239084, -0.01986224, -0.01484586], dtype=float32), array([ 0.01193139,  0.03211017, -0.00763281,  0.00524321], dtype=float32), array([-0.00538698, -0.00410172, -0.01355604, -0.01081002], dtype=float32), array([ 0.02732061, -0.80069524,  0.00247561], dtype=float32)]\n",
      "Step 20 | Reward: -1.000 | Terminated: False\n",
      "Step 21 | Reward: -1.000 | Terminated: False\n",
      "Step 22 | Reward: -1.000 | Terminated: False\n",
      "Step 23 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.01952537, -0.00102796, -0.01494487, -0.01937619], dtype=float32), array([ 0.01090394,  0.06191634,  0.02454281, -0.00128793], dtype=float32), array([ 0.00068169, -0.00439367, -0.0209519 , -0.02517723], dtype=float32), array([ 0.01356304, -0.58581215,  0.00267426], dtype=float32)]\n",
      "Step 24 | Reward: -1.000 | Terminated: False\n",
      "Step 25 | Reward: -1.000 | Terminated: False\n",
      "Step 26 | Reward: -1.000 | Terminated: False\n",
      "Step 27 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([-0.00203554,  0.03395263,  0.01380097, -0.00939796], dtype=float32), array([ 7.1268529e-05,  1.0580445e-01, -7.9445103e-03,  5.0896262e-03],\n",
      "      dtype=float32), array([ 0.00803101, -0.01503317, -0.02763931, -0.00763197], dtype=float32), array([ 0.06535207, -0.2967111 ,  0.01958608], dtype=float32)]\n",
      "Step 28 | Reward: -1.000 | Terminated: False\n",
      "Step 29 | Reward: -1.000 | Terminated: False\n",
      "Step 30 | Reward: -1.000 | Terminated: False\n",
      "Step 31 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.03641672,  0.02199886,  0.0185993 , -0.01803232], dtype=float32), array([ 0.00526643,  0.14322226, -0.00613959, -0.00537294], dtype=float32), array([ 0.01013985, -0.0138512 , -0.00151585,  0.00413037], dtype=float32), array([ 0.07893772, -0.40237832,  0.02329252], dtype=float32)]\n",
      "Step 32 | Reward: -1.000 | Terminated: False\n",
      "Step 33 | Reward: -1.000 | Terminated: False\n",
      "Step 34 | Reward: -1.000 | Terminated: False\n",
      "Step 35 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([0.00567442, 0.03511937, 0.00683665, 0.01008098], dtype=float32), array([-0.00292169,  0.08244579,  0.01588675,  0.00266372], dtype=float32), array([ 0.00407634, -0.0059124 ,  0.00786264, -0.03441985], dtype=float32), array([ 0.07187303, -0.16657466,  0.01050934], dtype=float32)]\n",
      "Step 36 | Reward: -1.000 | Terminated: False\n",
      "Step 37 | Reward: -1.000 | Terminated: False\n",
      "Step 38 | Reward: -1.000 | Terminated: False\n",
      "Step 39 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.04932112,  0.03294076, -0.03666678, -0.00277129], dtype=float32), array([ 0.00819761,  0.06938907, -0.02380858, -0.00225629], dtype=float32), array([ 0.00183494, -0.00217367, -0.01417915, -0.0430089 ], dtype=float32), array([ 0.05626048, -1.3897531 ,  0.01756888], dtype=float32)]\n",
      "Step 40 | Reward: -1.000 | Terminated: False\n",
      "Step 41 | Reward: -1.000 | Terminated: False\n",
      "Step 42 | Reward: -1.000 | Terminated: False\n",
      "Step 43 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.00383211,  0.00733373, -0.01380372,  0.00616946], dtype=float32), array([ 0.00802353,  0.00481669, -0.00851492, -0.00071767], dtype=float32), array([-0.00118752, -0.00290309, -0.02787173, -0.00853046], dtype=float32), array([0.00156389, 0.636229  , 0.81351227], dtype=float32)]\n",
      "Step 44 | Reward: -1.000 | Terminated: False\n",
      "Step 45 | Reward: -1.000 | Terminated: False\n",
      "Step 46 | Reward: -1.000 | Terminated: False\n",
      "Step 47 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.01482844,  0.00291785, -0.01637712, -0.00420705], dtype=float32), array([ 0.01433017, -0.00800375, -0.00483553, -0.00457333], dtype=float32), array([-0.00034981, -0.00267191, -0.03295324, -0.00454531], dtype=float32), array([-5.046462e-04,  6.910240e-01,  7.858425e-01], dtype=float32)]\n",
      "Step 48 | Reward: -1.000 | Terminated: False\n",
      "Step 49 | Reward: -1.000 | Terminated: False\n",
      "Step 50 | Reward: -1.000 | Terminated: False\n",
      "Step 51 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.01818657, -0.00162762, -0.01815267, -0.00966163], dtype=float32), array([ 0.01665234, -0.01372243, -0.00836079, -0.00497002], dtype=float32), array([-0.00087122, -0.00297714, -0.02257321, -0.00469662], dtype=float32), array([-0.00561341,  0.6552247 ,  0.71529436], dtype=float32)]\n",
      "Step 52 | Reward: -1.000 | Terminated: False\n",
      "Step 53 | Reward: -1.000 | Terminated: False\n",
      "Step 54 | Reward: -1.000 | Terminated: False\n",
      "Step 55 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.00815525,  0.0018429 , -0.01384015, -0.0021597 ], dtype=float32), array([ 0.01466948, -0.00597023, -0.00876785, -0.00352771], dtype=float32), array([-0.00022405, -0.00235173, -0.02116438,  0.00098299], dtype=float32), array([-0.00165052,  0.7241496 ,  0.8189029 ], dtype=float32)]\n",
      "Step 56 | Reward: -1.000 | Terminated: False\n",
      "Step 57 | Reward: -1.000 | Terminated: False\n",
      "Step 58 | Reward: -1.000 | Terminated: False\n",
      "Step 59 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.01969886, -0.00248995, -0.02466349, -0.00496678], dtype=float32), array([ 0.01850709, -0.00081301, -0.0137072 , -0.00376574], dtype=float32), array([-0.00142581, -0.00166888, -0.01468584,  0.00105076], dtype=float32), array([-0.00115562,  0.66860735,  0.66824913], dtype=float32)]\n",
      "Step 60 | Reward: -1.000 | Terminated: False\n",
      "Step 61 | Reward: -1.000 | Terminated: False\n",
      "Step 62 | Reward: -1.000 | Terminated: False\n",
      "Step 63 | Reward: -1.000 | Terminated: False\n",
      "Observation keys: dict_keys(['observation', 'achieved_goal', 'desired_goal'])\n",
      "[array([ 0.0160551 , -0.00307648, -0.02239842, -0.00623717], dtype=float32), array([ 0.01606213,  0.00345252, -0.00844192, -0.00419335], dtype=float32), array([-0.0006072 , -0.00207856, -0.01997726, -0.00063081], dtype=float32), array([-0.00272479,  0.6924441 ,  0.7344418 ], dtype=float32)]\n",
      "Step 64 | Reward: -1.000 | Terminated: False\n",
      "Step 65 | Reward: -1.000 | Terminated: False\n",
      "Step 66 | Reward: -1.000 | Terminated: False\n",
      "Step 67 | Reward: -1.000 | Terminated: False\n",
      "‚ùå Failure.\n",
      "Total reward: -68.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Video saved to: output_videos/rollout.mp4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import panda_gym\n",
    "import numpy as np\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output directory for video\n",
    "output_directory = Path(\"output_videos\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make environment\n",
    "env = gym.make(\"PandaPickAndPlace-v3\", render_mode=\"rgb_array\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Print environment spaces for sanity check\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# (Assuming `policy` is already loaded, e.g. via SmolVLAPolicy.from_pretrained(...) )\n",
    "print(\"Policy input features:\", policy.config.input_features)\n",
    "print(\"Policy output features:\", policy.config.output_features)\n",
    "\n",
    "# Reset\n",
    "policy.reset()\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "rewards = []\n",
    "frames = [env.render()]\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    # Debug observation structure\n",
    "    print(\"Observation keys:\", obs.keys())\n",
    "\n",
    "    # Convert observation to tensors\n",
    "    obs_raw = obs[\"observation\"]\n",
    "    state = torch.tensor(obs_raw[0:13], dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    goal = torch.tensor(obs[\"desired_goal\"], dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    img = torch.tensor(frames[-1], dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)/255\n",
    "\n",
    "    # Input format expected by policy\n",
    "    observation = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.images.image\": img,\n",
    "        \"task\": [\"pick and place\"]  # can be repeated per batch\n",
    "\n",
    "    }\n",
    "\n",
    "    # Get action\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Step\n",
    "    numpy_action = action.squeeze(0).cpu().numpy()\n",
    "    actions = [numpy_action[i:i+4] for i in range(0, len(numpy_action), 4)]\n",
    "    print(actions)\n",
    "    for action in actions:\n",
    "        if action.shape[0] == 4:\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        frames.append(env.render())\n",
    "\n",
    "        print(f\"Step {step} | Reward: {reward:.3f} | Terminated: {terminated}\")\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "# Final result\n",
    "print(\"üéØ Success!\" if terminated else \"‚ùå Failure.\")\n",
    "print(f\"Total reward: {sum(rewards):.2f}\")\n",
    "\n",
    "# Save video\n",
    "fps = env.metadata.get(\"render_fps\", 30)\n",
    "video_path = output_directory / \"rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), np.stack(frames), fps=fps)\n",
    "print(f\"üé• Video saved to: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = {\n",
    "        \"observation.state\": torch.from_numpy(np.random.rand(13)).unsqueeze(0).to(device,dtype=torch.float32),\n",
    "        \"observation.images.image\": torch.from_numpy(np.random.rand(3, 128, 128)).unsqueeze(0).to(device,dtype=torch.float32),\n",
    "        \"task\": [\"pick and place\"]  # can be repeated per batch\n",
    "\n",
    "    }\n",
    "\n",
    "# Get action\n",
    "with torch.inference_mode():\n",
    "    action = policy.select_action(observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887cdb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199, -0.0012, -0.0131, -0.0218,  0.0343,  0.0290,  0.0217, -0.0078,\n",
       "          0.0016, -0.0061,  0.0027, -0.0411,  0.0020, -0.9401, -0.0018]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e242a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
